{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "234cb352-b140-4b4a-9183-255100a03355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_utils.py\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import copy\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    '''\n",
    "    Usual Embedding layer with weights multiplied by sqrt(d_model)\n",
    "    '''\n",
    "    def __init__(self,embeddings ,d_model, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        #self.lut = nn.Embedding.from_pretrained(embeddings, freeze=True)\n",
    "        \n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() *\n",
    "                             -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(torch.as_tensor(position.numpy() * div_term.unsqueeze(0).numpy()))\n",
    "        pe[:, 1::2] = torch.cos(torch.as_tensor(position.numpy() * div_term.unsqueeze(0).numpy()))#torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + Variable(self.pe[:, :x.size(1)],\n",
    "                         requires_grad=False)\n",
    "        return self.dropout(x)\n",
    "# attention.py\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"Implementation of Scaled dot product attention\"\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    p_attn = F.softmax(scores, dim = -1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "\n",
    "    return torch.matmul(p_attn, value), p_attn\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), self.h)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"Implements Multi-head attention\"\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = query.size(0)\n",
    "\n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k\n",
    "        query, key, value = \\\n",
    "            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "             for l, x in zip(self.linears, (query, key, value))]\n",
    "\n",
    "        # 2) Apply attention on all the projected vectors in batch.\n",
    "        x, self.attn = attention(query, key, value, mask=mask,\n",
    "                                 dropout=self.dropout)\n",
    "\n",
    "        # 3) \"Concat\" using a view and apply a final linear.\n",
    "        x = x.transpose(1, 2).contiguous() \\\n",
    "             .view(nbatches, -1, self.h * self.d_k)\n",
    "        return self.linears[-1](x)\n",
    "class Encoder(nn.Module):\n",
    "    '''\n",
    "    Transformer Encoder\n",
    "\n",
    "    It is a stack of N layers.\n",
    "    '''\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    '''\n",
    "    An encoder layer\n",
    "\n",
    "    Made up of self-attention and a feed forward layer.\n",
    "    Each of these sublayers have residual and layer norm, implemented by SublayerOutput.\n",
    "    '''\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer_output = clones(SublayerOutput(size, dropout), 2)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"Transformer Encoder\"\n",
    "        x = self.sublayer_output[0](x, lambda x: self.self_attn(x, x, x, mask)) # Encoder self-attention\n",
    "        return self.sublayer_output[1](x, self.feed_forward)\n",
    "# sublayer.py\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    \"Construct a layer normalization module.\"\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
    "\n",
    "class SublayerOutput(nn.Module):\n",
    "    '''\n",
    "    A residual connection followed by a layer norm.\n",
    "    '''\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerOutput, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "# feed_forward.py\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Positionwise feed-forward network.\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"Implements FFN equation.\"\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))\n",
    "# Model.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, config, src_vocab):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.config = config\n",
    "\n",
    "        h, N, dropout = self.config.h, self.config.N, self.config.dropout\n",
    "        d_model, d_ff = self.config.d_model, self.config.d_ff\n",
    "\n",
    "        attn = MultiHeadedAttention(h, d_model)\n",
    "        ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "        position = PositionalEncoding(d_model, dropout)\n",
    "        self.src_vocab = src_vocab\n",
    "        self.encoder = Encoder(EncoderLayer(config.d_model, deepcopy(attn), deepcopy(ff), dropout), N)\n",
    "        \n",
    "        a = dataset.vocab.vectors.tolist()\n",
    "        for i in range(len(a)):\n",
    "            while(len(a[i])!=config.d_model):\n",
    "                a[i].append(0)\n",
    "                \n",
    "        a= torch.tensor(a)\n",
    "               \n",
    "        self.src_embed = nn.Sequential(Embeddings(dataset.vocab,config.d_model, len(self.src_vocab)), deepcopy(position)) #Embeddings followed by PE\n",
    "\n",
    "        # Fully-Connected Layer\n",
    "        self.fc = nn.Linear(\n",
    "            self.config.d_model,\n",
    "            self.config.output_size\n",
    "        )\n",
    "\n",
    "        # Softmax non-linearity\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self, x):    \n",
    "        \n",
    "        embedded_sents = self.src_embed(x.permute(1,0)) # shape = (batch_size, sen_len, d_model)\n",
    "        \n",
    "        encoded_sents = self.encoder(embedded_sents)\n",
    "\n",
    "        # Convert input to (batch_size, d_model) for linear layer\n",
    "        \n",
    "        \n",
    "        final_feature_map = encoded_sents[:,-1,:]\n",
    "\n",
    "\n",
    "        final_out = self.fc(final_feature_map)\n",
    "        final_out = self.softmax(final_out)\n",
    "        \n",
    "        return final_out\n",
    "\n",
    "    def add_optimizer(self, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def add_loss_op(self, loss_op):\n",
    "        self.loss_op = loss_op\n",
    "\n",
    "    def reduce_lr(self):\n",
    "        print(\"Reducing LR\")\n",
    "        for g in self.optimizer.param_groups:\n",
    "            g['lr'] = g['lr'] / 2\n",
    "    def predict(self,text):\n",
    "        y_pred = self.__call__(text)\n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "    def run_epoch(self, train_iterator, val_iterator, epoch):\n",
    "        train_losses = []\n",
    "        val_accuracies = []\n",
    "        losses = []\n",
    "\n",
    "        # Reduce learning rate as number of epochs increase\n",
    "\n",
    "\n",
    "        for i, batch in enumerate(train_iterator):\n",
    "\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                x = batch.text.cuda()\n",
    "                y = (batch.label-1).type(torch.LongTensor)\n",
    "                \n",
    "            else:\n",
    "                x = batch.text\n",
    "                y = (batch.label-1).type(torch.LongTensor)\n",
    "            \n",
    "            y_pred = self.__call__(x)\n",
    "\n",
    "            loss = self.loss_op(y_pred, y)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            losses.append(loss.data.cpu().numpy())\n",
    "            self.optimizer.step()\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                avg_train_loss = np.mean(losses)\n",
    "                train_losses.append(avg_train_loss)\n",
    "                print(\"\\tAverage training loss: {:.5f}\".format(avg_train_loss))\n",
    "                losses = []\n",
    "\n",
    "                # Evalute Accuracy on validation set\n",
    "                val_accuracy,f1 = evaluate_model(self, val_iterator)\n",
    "                print(\"\\tVal Accuracy: {:.4f}\".format(val_accuracy))\n",
    "                print(\"\\tf1: {:.4f}\".format(f1))\n",
    "                \n",
    "                self.train()\n",
    "\n",
    "        return train_losses, val_accuracies\n",
    "        \n",
    "    def run_epoch2(self, train_iterator,val_iterator,epoch):\n",
    "        losses = []\n",
    "        for j in range(epoch):\n",
    "            for i, batch in enumerate(train_iterator):\n",
    "                if torch.cuda.is_available():\n",
    "                    x = batch.text.cuda()\n",
    "                    y = (batch.label-1).type(torch.LongTensor)                \n",
    "                else:\n",
    "                    x = batch.text\n",
    "                    y = (batch.label-1).type(torch.LongTensor)\n",
    "\n",
    "                y_pred = self.__call__(x)\n",
    "                print([torch.argmax(y_pred, dim=1),y])\n",
    "                loss = self.loss_op(y_pred, y)\n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                loss.backward()\n",
    "                \n",
    "                self.optimizer.step()\n",
    "            losses.append(loss.detach().numpy())  \n",
    "            \n",
    "            if (j+1) % 1 == 0:\n",
    "                print (f'Epoch [{j+1}/{epoch}], Loss: {loss.item():.10f}')\n",
    "                # Evalute Accuracy on validation set\n",
    "                val_accuracy,f1 = evaluate_model(self, val_iterator)\n",
    "                print(\"\\tVal Accuracy: {:.4f}\".format(val_accuracy))\n",
    "                print(\"\\tf1: {:.4f}\".format(f1))\n",
    "                \n",
    "                self.train()\n",
    "                \n",
    "\n",
    "                \n",
    "        return losses\n",
    "# config.py\n",
    "\n",
    "class Config(object):\n",
    "    N = 6 #6 in Transformer Paper\n",
    "    d_model = 512 #512 in Transformer Paper\n",
    "    d_ff = 2048 #2048 in Transformer Paper\n",
    "    h = 8\n",
    "    dropout = 0.1\n",
    "    output_size = 4\n",
    "    lr = 0.0003\n",
    "    max_epochs = 200\n",
    "    batch_size = 1\n",
    "    max_sen_len = 50        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5120c213",
   "metadata": {},
   "outputs": [],
   "source": [
    "import telebot\n",
    "import time\n",
    "from telebot import types\n",
    "import json\n",
    "import torch\n",
    "\n",
    "token = '6705181314:AAH1F4h1C_rpM5pkcu3tXdeHkznDxIESz3o'\n",
    "bot = telebot.TeleBot(token, parse_mode='None')\n",
    "bot_name = 'ITMO BOT'\n",
    "\n",
    "model= torch.load('/Users/mac/Desktop/test/model.pth')\n",
    "def menu():\n",
    "        markup = types.InlineKeyboardMarkup()\n",
    "        markup.add(types.InlineKeyboardButton('help', callback_data='help'))   \n",
    "        markup.add(types.InlineKeyboardButton('contact', callback_data='contact'))   \n",
    "        markup.add(types.InlineKeyboardButton('main questions', callback_data='main questions'))  \n",
    "        markup.add(types.InlineKeyboardButton('more questions', callback_data='more questions')) \n",
    "        markup.add(types.InlineKeyboardButton('application', callback_data='application'))    \n",
    "        return markup\n",
    "def main():       \n",
    "    @bot.message_handler(commands=['start'])  # Ответ на команду /start\n",
    "    def start(message):\n",
    "        mess = f'hi, <b>{message.from_user.first_name}</b>!\\nI am - <b>{bot_name}</b>'\n",
    "        markup = menu()\n",
    "        bot.send_message(message.chat.id, mess, reply_markup=markup, parse_mode='html')\n",
    "\n",
    "def restart(message):\n",
    "    mess = f'hi, <b>{message.from_user.first_name}</b>!\\nI am - <b>{bot_name}</b>'\n",
    "    markup = menu()\n",
    "    bot.send_message(message.chat.id, mess, reply_markup=markup, parse_mode='html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8da70424-14cc-4e9b-b74c-a98cef6ffb41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requirement\n",
      "\n",
      "scholarships\n",
      "\n",
      "batchmates\n",
      "\n",
      "language\n",
      "\n",
      "research topics\n",
      "\n",
      "internship\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('/Users/mac/Desktop/SCIENTIFIC RESEARCH/main QA.json', 'r') as json_data:\n",
    "    main_intents = json.load(json_data)\n",
    "corpse = []\n",
    "responses = []\n",
    "for intent in main_intents['intents']:\n",
    "    tag = intent['tag']\n",
    "    response = intent['responses']\n",
    "    print(tag+\"\\n\")\n",
    "    corpse.append(tag)# here we are appending the word with its tag\n",
    "    responses.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1095a9d2-3f82-465f-af91-740ec26dbf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import regexp_tokenize\n",
    "@bot.callback_query_handler(func=lambda call: True)\n",
    "def message_reply(call):\n",
    "    if call.data == 'contact':\n",
    "        mess = 'click on mail to get contact with staff'\n",
    "        bot.send_message(call.message.chat.id,mess)\n",
    "        mess = \"\"\"\n",
    "Program coordinator --  aakarabintseva@itmo.ru\n",
    "International office -- international@itmo.ru\n",
    "Student office -- aakiseleva@itmo.ru\n",
    "Migration office -- aakhalilova@itmo.ru \n",
    "\"\"\"\n",
    "        bot.send_message(call.message.chat.id,mess)\n",
    "        time.sleep(5)\n",
    "        restart(call.message)\n",
    "    elif call.data == 'help': \n",
    "        mess = \"\"\"\n",
    "contact --  to find Email address of specific staff in ITMO\n",
    "main questions -- to answer most frequent questions from candidates\n",
    "more questions -- to answer other questions\n",
    "application -- to redirect to page to fill application\n",
    "\"\"\"\n",
    "        bot.send_message(call.message.chat.id,mess)\n",
    "        time.sleep(5)\n",
    "        restart(call.message)\n",
    "    elif call.data == 'main questions':\n",
    "        mess = 'please choose interested item'\n",
    "        markup = types.ReplyKeyboardMarkup(one_time_keyboard=True)\n",
    "        for i in range(len(corpse)):            \n",
    "            markup.add(corpse[i])  \n",
    "        msg = bot.send_message(call.message.chat.id,mess, reply_markup=markup)\n",
    "        bot.register_next_step_handler(msg, main_questions)\n",
    "    elif call.data == 'more questions':\n",
    "        msg = bot.send_message(call.message.chat.id, 'please write to questions')\n",
    "        bot.register_next_step_handler(msg, more_questions)\n",
    "    elif call.data == 'application':\n",
    "        linked_user = 'https://signup.itmo.ru/master'\n",
    "        markup = types.InlineKeyboardMarkup()\n",
    "        markup.add(types.InlineKeyboardButton(text='redirect to ITMO',\n",
    "                            url=linked_user))\n",
    "        mess = 'click to redirect to application form'\n",
    "        bot.send_message(call.message.chat.id,mess, reply_markup=markup)\n",
    "        time.sleep(5)\n",
    "        restart(call.message)\n",
    "    \n",
    "\n",
    "\n",
    "def main_questions(message):\n",
    "    def tokenize(sentence):\n",
    "        return regexp_tokenize(sentence, pattern=\"\\w+\")\n",
    "\n",
    "    def score_words(x,y):\n",
    "          \"\"\" returns the jaccard similarity between two lists \"\"\"\n",
    "          intersection_cardinality = len(set.intersection(*[set(x), set(y)]))\n",
    "          union_cardinality = len(set.union(*[set(x), set(y)]))\n",
    "          return intersection_cardinality/float(union_cardinality)\n",
    "    sentence = message.text\n",
    "    if(any(sentence.lower()==item.lower() for item in [\"quit\",\"finish\",\"over\",\"bye\",\"goodbye\"])):\n",
    "        print(f\"{bot_name}: Goodbye , have a nice day\")\n",
    "\n",
    "    similarity = []\n",
    "    for i in corpse:\n",
    "        similarity.append(score_words(sentence,i))\n",
    "    #print(similarity)\n",
    "    \n",
    "    if(max(similarity) > 0.5 and len(tokenize(sentence))==1 ):\n",
    "        print(f\"{bot_name}: \"+responses[similarity.index(max(similarity))][0])\n",
    "    \n",
    "    mess = responses[similarity.index(max(similarity))][0]\n",
    "    bot.send_message(message.chat.id, mess)\n",
    "\n",
    "    time.sleep(5)\n",
    "    restart(message)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ead392ff-e210-4af5-abad-e4469e5a3d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i am token\n",
      "['What', 'are', 'the', 'requirements', 'to', 'enter', 'the', 'program', '?']\n",
      "tensor([[  0,   9,   3, 190,   5,  47,   3,  12,   2,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1]])\n",
      "tensor([[9.9942e-01, 1.1392e-05, 1.7274e-05, 1.3223e-05, 3.8645e-07, 1.0744e-05,\n",
      "         1.8579e-06, 6.0522e-07, 3.6370e-05, 5.5307e-06, 2.1374e-07, 1.1548e-04,\n",
      "         1.4297e-04, 1.8410e-05, 1.1575e-05, 9.5544e-05, 4.4703e-05, 3.3011e-07,\n",
      "         8.2599e-07, 3.8206e-05, 2.2118e-07, 8.7527e-06, 2.1505e-07, 6.6277e-07]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor(0.9994, grad_fn=<MaxBackward1>)\n",
      "tensor([1])\n",
      "To enter the program applicants need to: – send a portfolio to the international department with their relevant professional (and academic) CV, transcript of records for their Bachelor's degree, and a proof of their English language proficiency; – pass one of the entrance trials (get a pass mark at the exam): an online theoretical exam on mathematics & programming / Portfolio contest for international applicants / OpenDoors Olympiad for international applicants.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/jupyterlab/4.0.6_1/libexec/lib/python3.11/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i am token\n",
      "['Are', 'all', 'courses', 'at', 'the', 'program', 'taught', 'in', 'English', '?']\n",
      "tensor([[  0, 109,  43,  17,   3,  12,  99,  15,   0,   2,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1]])\n",
      "tensor([[1.4977e-05, 4.4491e-04, 2.1037e-04, 9.9335e-01, 3.1562e-06, 1.8712e-03,\n",
      "         5.9387e-06, 5.8804e-04, 2.6249e-04, 2.7798e-06, 1.1834e-05, 8.3511e-06,\n",
      "         8.9419e-04, 1.2096e-04, 3.5883e-04, 1.3767e-04, 1.2635e-03, 8.4131e-05,\n",
      "         3.1195e-05, 2.1365e-05, 3.2253e-05, 9.0611e-06, 2.1210e-04, 6.1196e-05]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor(0.9933, grad_fn=<MaxBackward1>)\n",
      "tensor([4])\n",
      "Yes, 100% of courses offered at the program are taught in English.\n",
      "i am token\n",
      "['Will', 'I', 'get', 'the', 'chance', 'to', 'do', 'an', 'internship', 'during', 'my', 'studies', '?']\n",
      "tensor([[  0,   0,  23,   3, 121,   5,  14,  39,  33, 134,  21, 198,   2,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1]])\n",
      "tensor([[2.9050e-05, 2.0483e-05, 2.7716e-05, 4.8786e-05, 1.0792e-04, 9.9794e-01,\n",
      "         2.4958e-05, 4.4409e-06, 1.1618e-05, 4.4825e-05, 8.1000e-07, 3.3062e-05,\n",
      "         2.0873e-05, 1.3936e-05, 2.9603e-06, 1.8305e-05, 7.5419e-05, 6.2041e-05,\n",
      "         9.7472e-07, 1.7153e-05, 8.9161e-06, 1.4072e-03, 6.6333e-06, 7.7109e-05]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor(0.9979, grad_fn=<MaxBackward1>)\n",
      "tensor([6])\n",
      "Yes, the most convenient option would be to intern at one of our scientific labs where you will also have a chance to work on your Master's thesis. To know more about how to get internship , please contact the program manager via email: aakarabintseva@itmo.ru (Ms. Alexandra Karabintseva)\n",
      "i am token\n",
      "['can', 'i', 'get', 'intern']\n",
      "tensor([[13,  4, 23,  0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1]])\n",
      "tensor([[1.2676e-03, 3.2715e-01, 2.4291e-02, 1.6531e-04, 1.3360e-01, 1.5752e-01,\n",
      "         6.0777e-04, 4.3887e-04, 2.3677e-02, 4.8354e-03, 3.5251e-02, 1.4549e-01,\n",
      "         1.5069e-03, 2.4569e-02, 1.4782e-02, 2.0902e-04, 2.0383e-02, 9.5771e-03,\n",
      "         1.5344e-05, 6.2087e-06, 5.1325e-03, 4.6858e-02, 4.3943e-03, 1.8272e-02]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor(0.3272, grad_fn=<MaxBackward1>)\n",
      "tensor([2])\n",
      "We don't have our own scholarship funding within the program, but you can apply through the Russian government international scholarship website and choose our program in your application http://studyinrussia.ru/en/actual/scholarships/.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import random\n",
    "\n",
    "\n",
    "def more_questions(message):\n",
    "    def tokenize(sentence):\n",
    "        return regexp_tokenize(sentence, pattern=\"\\w+\")\n",
    "    sentence = message.text\n",
    "    NLP = spacy.blank(\"en\")\n",
    "    tokenizer = lambda sent: [x.text for x in NLP.tokenizer(sent) if x.text != \" \"]\n",
    "    token = tokenizer(sentence)\n",
    "    sequenced=[]\n",
    "    for i in range(len(token)):\n",
    "        sequenced.append(model.src_vocab[token[i]])\n",
    "    \n",
    "    while(len(sequenced)!=model.config.max_sen_len):\n",
    "        sequenced.append(1)\n",
    "    \n",
    "    array = np.array(sequenced)\n",
    "    \n",
    "    transposed_array = array.T\n",
    "    sequenced = transposed_array.tolist()\n",
    "    tensor = []\n",
    "    tensor.append(sequenced)\n",
    "    print('i am token')\n",
    "    print(token)\n",
    "    user_input = torch.tensor(tensor)\n",
    "    print(user_input)\n",
    "    user_input = user_input.permute(1,0)\n",
    "    \n",
    "    output = model.predict(user_input)\n",
    "    print(output)\n",
    "    prob = output.max() \n",
    "    print(prob)\n",
    "    output = torch.argmax(output, dim=1)\n",
    "    \n",
    "    output = output + 1\n",
    "    print(output)\n",
    "    mess = ''\n",
    "    with open('/Users/mac/Desktop/test/test.json', 'r') as json_data:\n",
    "        intents = json.load(json_data)\n",
    "    if prob > 0.1:\n",
    "            for intent in intents['intents']:\n",
    "                if output == intent[\"tag\"]:\n",
    "                    mess = random.choice(intent['responses'])\n",
    "                    print(random.choice(intent['responses']))\n",
    "                \n",
    "    else:\n",
    "        mess = \"Sorry I am unable to Process Your Request\"\n",
    "        mess += \"You may find the way forward in https://en.itmo.ru/en/viewjep/2/5/Big_Data_and_Machine_Learning.htm\"\n",
    "    \n",
    "    bot.send_message(message.chat.id, mess)\n",
    "    time.sleep(5)\n",
    "    restart(message)\n",
    "if __name__ == \"__main__\":\n",
    "        main()\n",
    "        bot.polling(none_stop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc7c0b5-5cdd-451f-af55-8e2691102e53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
